{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-apartment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "db = wrds.Connection(wrds_username = 'cmvj22')\n",
    "# Require subscription to wrds. All Durham staff and students may request access via the University.\n",
    "\n",
    "\n",
    "np.set_printoptions(edgeitems = 9, linewidth = 128,\n",
    "                    formatter = {'float':lambda x: f'{x:.3f}'}, precision = 3, suppress = True)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "#############################################################################\n",
    "# SSH is faster than JupyterLab which is faster than normal Jupyter         #\n",
    "#############################################################################\n",
    "# On 2 cores on SSH Cloud, about 2/3rds take <15mins so run 2 or 3 and wait\n",
    "# 15 mins for one to finish first. The rest take 60 mins, at random.\n",
    "\n",
    "def get_sql(statement, date_col):\n",
    "    res = db.raw_sql(statement)\n",
    "    res[f'{date_col}'] = res[f'{date_col}'].apply(lambda x: pd.Timestamp(x))\n",
    "    return res\n",
    "\n",
    "def save_reopen_no_num(var, name, printed=True):\n",
    "    var.to_csv(f'{name}.csv', index=False)\n",
    "    if printed:\n",
    "        print(f'{name} done at time: {time.strftime(\"%H:%M:%S\", time.gmtime())}')\n",
    "    return pd.read_csv(f'{name}.csv')#, parse_dates=[f'{date_col}'])\n",
    "\n",
    "# Strikes are reported x1000 and contract is 100 shares\n",
    "def contract_ids(capEforEuro, newYearDay, newYearEve):\n",
    "    contracts_statement = f\"\"\"SELECT optionid, securityid, strike/100000 as strike, expiration, callput\n",
    "                              FROM optionm.option_history\n",
    "                              WHERE callput in ('C')\n",
    "                                  AND exercisestyle = '{capEforEuro}'\n",
    "                                  AND startdate BETWEEN '{newYearDay}' AND '{newYearEve}'\n",
    "                              ORDER BY optionid ASC\"\"\"\n",
    "    return get_sql(contracts_statement, 'expiration')\n",
    "\n",
    "# Contracts are for 100 shares\n",
    "def option_prices(year, contracts):\n",
    "    all_prices_statement = f\"\"\"SELECT optionid, date AS date_traded, last/100 AS contract_price,\n",
    "                                          underlyinglast/100 AS underlyings_price, volume AS contract_volume\n",
    "                               FROM optionm.option_price_{year}\n",
    "                               WHERE optionid in {tuple(contracts['optionid'])}\n",
    "                                   AND volume > 2\n",
    "                                   AND last > 5\n",
    "                                   AND last < 40\n",
    "                                   AND underlyinglast > 0.5\n",
    "                                   AND specialsettlement = 0\n",
    "                               GROUP BY optionid, date, volume, last, underlyinglast\n",
    "                               ORDER BY optionid, date\"\"\"\n",
    "    return get_sql(all_prices_statement, 'date_traded')\n",
    "\n",
    "def combine_option_info(contracts, all_prices, combined):\n",
    "    combined_1yr = pd.merge(contracts, all_prices, how = 'right', on = 'optionid')\n",
    "    combined_1yr = combined_1yr.sort_values(by = ['optionid'], ignore_index = True)\n",
    "    combined_1yr['days_to_maturity'] = (combined_1yr['expiration'] - combined_1yr['date_traded']) / np.timedelta64(1,'D')# This might be optimisable\n",
    "    combined_1yr = combined_1yr[(1 < combined_1yr['days_to_maturity']) & (combined_1yr['days_to_maturity'] < 366)] # Only removes 3.5% of rows\n",
    "    combined_1yr.drop(columns = ['expiration'], inplace = True)\n",
    "    return pd.concat([combined, combined_1yr], axis = 0, ignore_index = True)\n",
    "\n",
    "def toosmall(vec):\n",
    "    if int(np.sum(vec)) == 0:\n",
    "        print('Number too small - not enough quotes for splicing rates')\n",
    "        print('Exiting script.')\n",
    "        sys.exit()\n",
    "\n",
    "print(f'Start time: {time.strftime(\"%H:%M:%S\", time.gmtime())}')\n",
    "start = time.time()\n",
    "\n",
    "################################################################################\n",
    "# Info on all trades for num option (Euro Call) contracts each year 2002-2019. #\n",
    "################################################################################\n",
    "\n",
    "combined      = pd.DataFrame()\n",
    "contract_list = []\n",
    "num_checked   =  pd.DataFrame()\n",
    "\n",
    "for year in range(2002,2020):\n",
    "    newYearDay = f'{year}-01-01'\n",
    "    newYearEve = f'{year}-12-31'\n",
    "    \n",
    "    contracts = contract_ids('E', newYearDay, newYearEve)\n",
    "    # A contract may be traded in the next calendar year so will check for last two years' optionids.\n",
    "    contract_list.append(contracts)\n",
    "    if year != 2002:\n",
    "        contracts = pd.concat([contract_list.pop(0), contracts], axis = 0, ignore_index = True)\n",
    "    num_checked = pd.concat([num_checked, contracts.optionid], axis = 0, ignore_index = True)\n",
    "                            \n",
    "    all_prices = option_prices(year, contracts)\n",
    "    combined   = combine_option_info(contracts, all_prices, combined)\n",
    "                            \n",
    "combined.sort_values(by = ['securityid', 'callput', 'optionid', 'date_traded'], inplace = True, ignore_index = True)\n",
    "num_checked = len(num_checked[0].unique())\n",
    "# This ratio is sufficient for pricing so removes a feature for the network.\n",
    "combined['moneyness'] = combined['underlyings_price'] / combined['strike']\n",
    "\n",
    "#######################################################################################\n",
    "# Adding interest rates (matching maturity and start date) for every options contract #\n",
    "#######################################################################################\n",
    "\n",
    "# Rates section below\n",
    "\n",
    "# In addition to maturities outside of our (2,365) range, the unused maturities inside this range are\n",
    "# collected as well to help make splicing more accurate and allow more date rows to be spliced.\n",
    "interest_statement = f\"\"\"SELECT date AS date_traded, days AS days_to_maturity, rate\n",
    "                         FROM optionm.zerocd\n",
    "                         WHERE days BETWEEN 1 AND 373\n",
    "                             AND date in {tuple(combined.date_traded.drop_duplicates().dt.strftime('%Y-%m-%d'))}\n",
    "                         GROUP BY date, days, rate\n",
    "                         ORDER BY date, days\"\"\"\n",
    "interests = get_sql(interest_statement, 'date_traded')\n",
    "\n",
    "# One row for every maturity per date including buffer and unused maturities to help splicing\n",
    "repeated_dates = pd.DataFrame({'date_traded': np.repeat(interests.date_traded.unique(), 373)})\n",
    "# Each date has one of every possible maturities\n",
    "repeated_maturities = pd.DataFrame({ 'days_to_maturity': np.tile(np.linspace(1,373,373), len(interests.date_traded.unique()) ) })\n",
    "# Every date has every maturity length\n",
    "dated_maturities = pd.concat([repeated_dates, repeated_maturities], axis = 1)\n",
    "# All known rate values for all date-maturity rows\n",
    "all_rates = pd.merge(interests, dated_maturities, how = 'right', on = ['date_traded', 'days_to_maturity'])\n",
    "\n",
    "# All available rates for all dates with enough rates to be spliced (most non-Fridays have at least 4 rates)\n",
    "non_nan_counts = all_rates.groupby('date_traded')['rate'].apply( lambda vec: (len(vec)-np.isnan(vec).sum()) > 3 )\n",
    "toosmall(non_nan_counts)\n",
    "all_rates_purged = all_rates[np.repeat(non_nan_counts.values, 373)]\n",
    "\n",
    "# Cubic-splice interpolating rates to all required maturities\n",
    "def splicer(vec):\n",
    "    return vec.interpolate(method = 'spline', order = 3, s = 0., limit_direction = 'both')\n",
    "inter_rates = all_rates_purged.set_index('days_to_maturity')\n",
    "inter_rates = pd.DataFrame(inter_rates.groupby('date_traded')['rate'].apply(splicer))\n",
    "inter_rates.reset_index(level = 0, inplace = True)\n",
    "\n",
    "req_rates_dated = pd.merge(all_rates_purged.reset_index(drop = True).drop(columns = ['rate']),\n",
    "                           inter_rates, left_index = True, right_index = True)\n",
    "req_rates_dated.drop(columns = ['days_to_maturity_x'], inplace = True)\n",
    "req_rates_dated.rename(columns = {'days_to_maturity_y': 'days_to_maturity'}, inplace = True)\n",
    "\n",
    "#Discard maturities not corresponding to option prices\n",
    "req_rates_dated = req_rates_dated[req_rates_dated.days_to_maturity.isin(np.sort(combined.days_to_maturity.unique()))].reset_index(drop=True)\n",
    "\n",
    "# Remove any dates where the rate points led to extreme rate values from inter/extrapolation\n",
    "lim_min = min(interests.rate)\n",
    "lim_max = max(interests.rate)\n",
    "well_spliced_dates = req_rates_dated.groupby('date_traded', as_index = False)['rate'].apply(\n",
    "                        lambda vec: (lim_min < min(vec)) & (max(vec) < lim_max) ).drop(columns = ['date_traded'])\n",
    "toosmall(well_spliced_dates)\n",
    "req_rates_dated = req_rates_dated[well_spliced_dates.rate.repeat(len(combined.days_to_maturity.unique())).reset_index(drop=True)]\n",
    "# Rates were reported as percentages\n",
    "req_rates_dated['rate'] = req_rates_dated['rate']/100\n",
    "\n",
    "####################################################################\n",
    "# Join 'combined' and 'req_rates_dated' for a single complete table#\n",
    "####################################################################\n",
    "\n",
    "data = pd.merge(combined, req_rates_dated, on = ['date_traded', 'days_to_maturity'])\n",
    "\n",
    "# Company mergers lead to existing contracts being quoted for both securityid's.\n",
    "data.drop_duplicates(subset = data.columns.difference(['securityid']), ignore_index = True, inplace = True)\n",
    "# Some repetition of quotes where only difference is strike being slightly changed.\n",
    "data.drop_duplicates(subset = data.columns.difference(['strike']), ignore_index = True, inplace = True)\n",
    "# Calculation of days_to_maturity has two answers, typically one day off.\n",
    "data.drop_duplicates(subset = data.columns.difference(['days_to_maturity', 'rate']), ignore_index = True, inplace = True)\n",
    "# Some calls are duplicated -- Not sure why.\n",
    "data.drop_duplicates(subset = data.columns.difference(['optionid']), ignore_index = True, inplace = True)\n",
    "# Some quotes duplicated across exchanges.\n",
    "data.drop_duplicates(subset = data.columns.difference(['optionid', 'contract_volume']), ignore_index = True, inplace = True)\n",
    "\n",
    "\n",
    "# Remove any companys with dividends or stock splits (62% of companies but 1.4% of quotes including puts)\n",
    "data = data[~data['securityid'].isin(db.get_table('optionm', 'distribution').securityid.unique())]\n",
    "# There are some outlier contracts which are either accidental trades or the strike/quote multiplier was missed\n",
    "data = data[(0.5 < data['moneyness']) & (data['moneyness'] < 1.5)]\n",
    "\n",
    "###############################################################################\n",
    "# Adding historical realised volatility for 30 days prior to each data_traded #\n",
    "###############################################################################\n",
    "\n",
    "vol_statement = f\"\"\"SELECT securityid, date AS date_traded, volatility\n",
    "                    FROM optionm.historical_volatility\n",
    "                    WHERE days = 30\n",
    "                        AND securityid in {tuple(data.securityid.unique())}\n",
    "                        AND date in {tuple(data.date_traded.drop_duplicates().dt.strftime('%Y-%m-%d'))}\n",
    "                    GROUP BY securityid, date, volatility\n",
    "                    ORDER BY securityid, date\"\"\"\n",
    "vols = get_sql(vol_statement, 'date_traded')\n",
    "data = pd.merge(data, vols, how = 'left').dropna() # A small handful of dates don't have data, hence .dropna()\n",
    "data = data[data.volatility < 2] # Market stress causes severe outliers for the Black-Scholes accuracy benchmark\n",
    "\n",
    "###############################################\n",
    "#                 # Summary #                 #\n",
    "###############################################\n",
    "\n",
    "num_contracts = len(data.optionid.unique())\n",
    "num_quotes    = len(data.index)\n",
    "num_calls     = len(data[data['callput'] == 'C'])\n",
    "num_puts      = len(data.index) - num_calls\n",
    "\n",
    "data = data[['strike', 'moneyness', 'rate', 'volatility', 'days_to_maturity', 'contract_price']]\n",
    "data = save_reopen_no_num(data, f'calls_OMrates{len(data.index)}')\n",
    "\n",
    "\n",
    "print(f'End time: {time.strftime(\"%H:%M:%S\", time.localtime())}')\n",
    "print(f'Time taken: {time.time() - start:.1f}')\n",
    "\n",
    "if True: print(f\"\"\"\n",
    "\n",
    "This script uses the OptionMetrics interest rates.\n",
    "These are inconsistent with put-call parity so only\n",
    "real calls are returned. Another script uses\n",
    "put-call parity to deduce the rate.\n",
    "\n",
    "For 2002 to 2019 inclusive, this database has\n",
    "3,301,638 positive-volume quotes on\n",
    "1,267,300 distinct Euro call contracts.\n",
    "\n",
    "This script checked {num_checked:,} Euro-call contracts\n",
    "and after removing unsuitable quotes, we returned\n",
    "{num_quotes:,} quotes from {num_contracts:,} contracts;\n",
    "{100*num_quotes/(3301638):,.1f}% and {100*num_contracts/(1267300):,.1f}% of quotes and contracts respectively.\n",
    "All are converted to call prices in the output file.\n",
    "\n",
    "Two-thirds of quotes are unfortunately filtered out\n",
    "because the Black-Scholes accuracy benchmark performs\n",
    "exceptionally poorly for quotes with very large volatility\n",
    "or prices near the ticksize so these are removed as\n",
    "the error appears systematic (extreme market stress on\n",
    "particular days rather than the volatility smile itself).\n",
    "\n",
    "Also, this database is truly enormous: there are\n",
    "1,096,159,594 total quotes for 2002-2019,\n",
    "107,988,528 of which are for Euro calls.\"\"\")\n",
    "\n",
    "\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
