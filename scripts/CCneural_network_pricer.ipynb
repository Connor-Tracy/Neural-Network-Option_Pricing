{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b6d42a",
   "metadata": {},
   "source": [
    "<font color=pink>\n",
    "For HyperParameter search, it can be necessary to ensure no other operations on GPU, in particular other Jupyter kernels. Run ```nvidia-smi``` in terminal to check.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59394763",
   "metadata": {},
   "source": [
    "<font color=pink>\n",
    "TensorFlow makes log folder and file names so long that it exceeds the 260 character address length limit.\n",
    "<br>\n",
    "This file should be run from a directory with as short an address as possible. Tested with TF 2.3.\n",
    "<br>\n",
    "Also advisable to pause cloud storage sinking (i.e. OneDrive) as this can block the logging of trial results.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61ef80",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "#          Initialisation           #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc35a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow.keras.backend as kb\n",
    "import kerastuner as kt\n",
    "from tensorflow import math as tm\n",
    "print(tf.config.list_physical_devices()) # Check TensorFlow can see your GPU.\n",
    "\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "np.set_printoptions(edgeitems=9, linewidth=128,\n",
    "                    formatter={'float':lambda x: f'{x:.3f}'}, precision=3, suppress=True)\n",
    "\n",
    "# To run on CPU:\n",
    "tf.config.set_visible_devices(tf.config.experimental.list_physical_devices('CPU'))\n",
    "\n",
    "# Rounding errors on float32 can inflate the loss\n",
    "floatsize = 'float64'\n",
    "kb.set_floatx(floatsize)\n",
    "\n",
    "market     = True                           # False if training set is Heston prices, True if market data.\n",
    "dim        = 1                              # Dim only relevant if Market==False, unused otherwise. (=1,4,7,10,13,16)\n",
    "randSearch = True                           # Enable if using random search or hard-coding hyperparameters.\n",
    "hyp_epochs = 30\n",
    "fit_epochs = 100\n",
    "size       = 6*10**5\n",
    "batch      = 2**8\n",
    "val_split  = 0.2\n",
    "max_trials = 1\n",
    "\n",
    "if market:\n",
    "    dim_name = 'market_acc'\n",
    "    features = 5                            # 5 input features: strike, moneyness, rate, volatility, maturity.\n",
    "else:\n",
    "    dim_name = f'heston_dim{dim}'\n",
    "    features = 7 + 2 * dim                  # 7+2*dim input features: dim spot+corr vectors, plus market stats.\n",
    "\n",
    "\n",
    "def df(size = 6*10**5, market = market, dim = dim):\n",
    "    if market:        \n",
    "        data = pd.read_csv('.\\\\data\\\\calls_OMrates587569.csv')\n",
    "        data = data[['strike', 'moneyness', 'rate', 'volatility', 'days_to_maturity', 'contract_price']]\n",
    "    else:\n",
    "        data = pd.read_csv(f'.\\\\data\\\\heston_prices\\\\heston_prices_dim{dim:02}.csv', header=None)\n",
    "    size0 = int(min(size, len(data)))\n",
    "    data  = data.sample(n = size0, replace = False)\n",
    "      \n",
    "    inputs = data[data.columns[:-1]].to_numpy()\n",
    "    prices = data[data.columns[-1]].to_numpy()\n",
    "    return inputs, prices.reshape(size0, 1)\n",
    "    \n",
    "def pretty_hparams(hparams):\n",
    "    keys       = hparams.keys()\n",
    "    max_length = max([len(key) for key in keys])\n",
    "    indices    = ['number_layers', 'number_units', 'learning_rate', 'rate_decay',\n",
    "                  'l1_regularizer', 'l2_regularizer', 'activation_func']\n",
    "    for key, value in hparams.items():\n",
    "        print(f'{key:<15} : {value}')\n",
    "    print('')\n",
    "\n",
    "# Terminate training after patience^th consecutive epoch with non-decreasing val_loss\n",
    "early_stopR = keras.callbacks.EarlyStopping(monitor='val_loss', patience= 20) # Random search\n",
    "early_stopB = keras.callbacks.EarlyStopping(monitor='val_loss', patience= 20) # Bayesian search\n",
    "early_stopT = keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5)  # Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a14ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hparams):\n",
    "    if randSearch:\n",
    "        indices = hparams.keys()\n",
    "    else:\n",
    "        indices = ['number_layers', 'number_units', 'learning_rate', 'rate_decay',\n",
    "                   'l1_regularizer', 'l2_regularizer', 'activation_func']\n",
    "    number_layers, number_units, learning_rate, rate_decay, l1_reg, l2_reg, a_func = [hparams[ind] for ind in indices]\n",
    "\n",
    "    initializer = tf.keras.initializers.he_uniform()#normal()    # All networks use ELU so we use He weight normalisation\n",
    "    regularizer = keras.regularizers.l1_l2(l1_reg, l2_reg)\n",
    "\n",
    "    norm_layer = Normalization()\n",
    "    norm_layer.adapt(df()[0])\n",
    "    model = keras.Sequential()\n",
    "    model.add( InputLayer(input_shape=(features,), name='Input_Layer') )\n",
    "    \n",
    "    for layer in range(number_layers):\n",
    "        model.add( Dense( units             = number_units,\n",
    "                         name               = f'Layer_{layer+1}',\n",
    "                         kernel_initializer = initializer,           # Initialize weights with He/Xavier initialiser\n",
    "                         kernel_regularizer = regularizer,\n",
    "                         activation         = a_func,\n",
    "                         dtype              = floatsize) )\n",
    "        #model.add( BatchNormalization(renorm = True) )              # Batch-norm in TF requires float32, cannot accept float64.\n",
    "        #model.add( Dropout(0.2) )\n",
    "        \n",
    "    model.add( Dense(units = 1, activation = 'linear',               # Output is a single scalar (price)\n",
    "                     dtype = floatsize, name='Output_Layer') )\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        learning_rate, decay_steps = 4000, decay_rate = rate_decay, staircase = True)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "        loss      = tf.keras.losses.MeanAbsolutePercentageError(), # Train, test loss & objective.\n",
    "        metrics   = [tf.keras.metrics.MeanSquaredError()])\n",
    "# Metrics report values but are not used in training. MAPE is our objective loss function.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b73b8",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "# Random Search\n",
    "<br>\n",
    "<font color=green>\n",
    "(TensorBoard hparams plugin hp module)     \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6f315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_range  = [2,4]\n",
    "node_range   = [5, 9]\n",
    "l_rate_range = (np.linspace(10**(-5), 0.003, 400)).tolist()\n",
    "decay_range  = (np.linspace(0.85, 0.9995, 400)).tolist()\n",
    "reg1_range   = (np.linspace(10**(-7.5), 10**(-5.5), 400)).tolist()\n",
    "reg2_range   = (np.linspace(10**(-7), 10**(-4.5), 400)).tolist()\n",
    "act_funcs    = ['elu']\n",
    "# (10**(np.linspace(-9, -3, 600))).tolist() to sample log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36145243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HP_LAYERS = hp.HParam('number_layers', hp.IntInterval(*layer_range))\n",
    "HP_UNITS  = hp.HParam('number_units',  hp.IntInterval(*node_range))\n",
    "HP_LRATE  = hp.HParam('learning_rate',   hp.Discrete(l_rate_range))\n",
    "HP_LDECAY = hp.HParam('rate_decay',      hp.Discrete(decay_range))\n",
    "HP_REGL1  = hp.HParam('l1_regularizer',  hp.Discrete(reg1_range))\n",
    "HP_REGL2  = hp.HParam('l2_regularizer',  hp.Discrete(reg1_range))\n",
    "HP_ACTS   = hp.HParam('activation_func', hp.Discrete(act_funcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2d77ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Starting trial randTrial-1\n",
      "number_layers   : 3\n",
      "number_units    : 5\n",
      "learning_rate   : 0.0003547117794486216\n",
      "rate_decay      : 0.9223145363408521\n",
      "l1_regularizer  : 3.15443140732736e-06\n",
      "l2_regularizer  : 1.6087195976465604e-06\n",
      "activation_func : elu\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Layer_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 96\n",
      "Trainable params: 96\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Average val loss:  45.98911875111771\n",
      "Trial time taken:  37.22319984436035\n",
      "Cumulative time taken: 37.22419095039368\n",
      "\n",
      "Total time taken:  37.225191593170166\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "trial_num = 1\n",
    "for i in range(max_trials):\n",
    "    startrun       = time.time()\n",
    "    run_name       = f'randTrial-{trial_num}'\n",
    "    logdir         = f\"logs\\\\random_search\\\\{dim_name}\\\\{dim_name}\" + datetime.now().strftime(\"%d-%H%M-%S\") + \"\\\\\" + run_name\n",
    "    inputs, prices = df(size, market, dim)\n",
    "    hyperparams = {\n",
    "        'number_layers'   : HP_LAYERS.domain.sample_uniform(),\n",
    "        'number_units'    : HP_UNITS.domain.sample_uniform(),\n",
    "        'learning_rate'   : HP_LRATE.domain.sample_uniform(),\n",
    "        'rate_decay'      : HP_LDECAY.domain.sample_uniform(),\n",
    "        'l1_regularizer'  : HP_REGL1.domain.sample_uniform(),\n",
    "        'l2_regularizer'  : HP_REGL2.domain.sample_uniform(),\n",
    "        'activation_func' : HP_ACTS.domain.sample_uniform() }\n",
    "    \n",
    "    print(f'----Starting trial {run_name}')\n",
    "    pretty_hparams(hyperparams)\n",
    "    \n",
    "    model = model_builder(hyperparams)\n",
    "    print(model.summary())\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(logdir), early_stopR,     # MSE+loss\n",
    "                 hp.KerasCallback(logdir+\"\\\\validation\", hyperparams)]    # TB Hparams\n",
    "    \n",
    "    train_history = model.fit(inputs, prices,\n",
    "                              validation_split = val_split,\n",
    "                              batch_size       = batch,\n",
    "                #              callbacks        = callbacks,\n",
    "                              shuffle          = True,\n",
    "                              verbose          = 0,\n",
    "                              epochs           = hyp_epochs)\n",
    "    \n",
    "    print('Average val loss: ', np.average(train_history.history['val_loss'][-3:]))\n",
    "    print('Trial time taken: ', time.time()-startrun)\n",
    "    print('Cumulative time taken:', time.time()-start)\n",
    "    print('')\n",
    "    trial_num += 1\n",
    "\n",
    "print('Total time taken: ', time.time()-start)\n",
    "    \n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(1000, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c34932",
   "metadata": {},
   "source": [
    "## # Bayesian search using Keras Tuner\n",
    "<br>\n",
    "<font color=green>\n",
    "(keras-tuner HyperParameters hy_Params)     \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "randSearch = False\n",
    "hy_Params = HyperParameters()\n",
    "hy_Params.Float(  'learning_rate',  *[10**(-3), 0.01] )\n",
    "hy_Params.Float(  'rate_decay',     *[0.85, 0.9995] )\n",
    "hy_Params.Float(  'l1_regularizer', *[10**(-8), 10**(-6.5)] )        #, sampling='LOG' also an option\n",
    "hy_Params.Float(  'l2_regularizer', *[10**(-8), 10**(-6.5)] )\n",
    "hy_Params.Choice( 'activation_func', ['elu'] )\n",
    "hy_Params.Int( 'number_layers', step=1, *[2, 6] )\n",
    "hy_Params.Int( 'number_units',  step=1, *[4, 7]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6150403",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logdir = f\"logs\\\\Bayes_search\\\\{dim_name}\\\\\" + datetime.now().strftime(\"%d-%H%M\")\n",
    "\n",
    "tuner = kt.BayesianOptimization(model_builder,\n",
    "                                hyperparameters = hy_Params,\n",
    "                                max_trials      = max_trials,\n",
    "                                objective       = 'val_loss',\n",
    "                                directory       = logdir,\n",
    "                                project_name    = \"proj\" + datetime.now().strftime(\"%d-%H%M\"))\n",
    "# The logging here saves each trial but does not itself report to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e02049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once complete, only retain summary of best trial\n",
    "import IPython\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)\n",
    "        print('Cumulative time taken:', time.time()-start)\n",
    "\n",
    "# For Bayesian optimisation we do not need to visualise the HPARAMS\n",
    "# in TensorBoard but we can still log the training curves of each run.\n",
    "cb_dir     = logdir + \"bayes_trial\" + datetime.now().strftime(\"%d-%H%M\") + \"\\\\\"\n",
    "callbacks  = [early_stopB, ClearTrainingOutput(), tf.keras.callbacks.TensorBoard(cb_dir)]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c928f25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "inputs, prices = df(size, market, dim)\n",
    "\n",
    "tuner.search(inputs, prices,\n",
    "             validation_split = val_split,\n",
    "             batch_size       = batch,\n",
    "             callbacks        = callbacks,\n",
    "             verbose          = 0,\n",
    "             initial_epoch    = 0,\n",
    "             shuffle          = True,\n",
    "             epochs           = hyp_epochs)\n",
    "                             \n",
    "print('Time Taken: ', time.time()-start)\n",
    "print(' ')\n",
    "    \n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(1000, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ad354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_results = min(max_trials,10)\n",
    "bests = tuner.get_best_hyperparameters(num_results)\n",
    "for i in range(num_results):\n",
    "    sup = (['st', 'nd', 'rd'] + ['th'] * 7) * 5\n",
    "    sup[10:13] = ['th'] * 3\n",
    "    print(f'{i+1}{sup[i]} best hyperparameter configuration found:')\n",
    "    pretty_hparams(bests[i].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51069e81",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "# Train model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fc6fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss:  32.33586105114325\n",
      "Average val loss:  33.504411792813386\n",
      "Time taken:  60.92910432815552\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "if market:\n",
    "    logdir = f'.\\\\trained_models_and_logs\\\\accuracy\\\\' + datetime.now().strftime(\"%d-%H%M\")\n",
    "else:\n",
    "    logdir = f'.\\\\trained_models_and_logs\\\\efficiency\\\\heston_dim{dim:02}\\\\'\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=logdir+'logs', update_freq='epoch')\n",
    "\n",
    "if randSearch:\n",
    "    best_hp = { # Copy hparams from printout then add string-quotes and commas.\n",
    "        'number_layers'   : 3,\n",
    "        'number_units'    : 12,\n",
    "        'learning_rate'   : 0.004866422595439956,\n",
    "        'rate_decay'      : 0.8814231084354753,\n",
    "        'l1_regularizer'  : 8.01608844764161e-09,\n",
    "        'l2_regularizer'  : 2.4880966570023003e-08,\n",
    "        'activation_func' : 'elu' }\n",
    "else:\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "model          = model_builder(best_hp)\n",
    "inputs, prices = df(size, market, dim)\n",
    "train_history  = model.fit(inputs, prices,\n",
    "                           validation_split = val_split,\n",
    "                           batch_size       = batch,\n",
    "                           callbacks        = [tensorboard_cb, early_stopT],\n",
    "                           shuffle          = True,\n",
    "                           verbose          = 0,\n",
    "                           epochs           = fit_epochs)\n",
    "                             \n",
    "print(\"Average training loss: \", np.average(train_history.history['loss'][-3:]))\n",
    "print(\"Average val loss: \", np.average(train_history.history['val_loss'][-3:]))\n",
    "print('Time taken: ', time.time()-start)\n",
    "\n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(1000, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32191cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average val mse:  0.008916552919599863\n"
     ]
    }
   ],
   "source": [
    "print(\"Average val mse: \", np.average(train_history.history['val_mean_squared_error'][-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0061198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(logdir)\n",
    "#saved_model = keras.models.load_model(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261f3bc",
   "metadata": {},
   "source": [
    "The below is only for visual reference and not the reported test loss, since this includes training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes(arr): # Returns analytical B-S prices for each row of inputs in df\n",
    "    arr    = pd.DataFrame(arr, columns=['strike', 'moneyness', 'rate', 'volatility', 'days_to_maturity'])\n",
    "    N      = lambda vec: norm.cdf(vec)\n",
    "    mats   = arr.days_to_maturity / 365\n",
    "    d1_num = np.log(arr.moneyness) + (arr.rate + 0.5 * arr.volatility ** 2) * mats\n",
    "    d1_den = arr.volatility * mats ** 0.5\n",
    "    d1     = d1_num / d1_den\n",
    "    d2     = d1 - d1_den\n",
    "    return arr.strike * ( N(d1) * arr.moneyness - N(d2) * np.exp(-arr.rate * mats ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37216862",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 15\n",
    "preds = model.predict(inputs[:num_predictions].reshape(num_predictions,features))\n",
    "truth = prices[:num_predictions]\n",
    "if market:\n",
    "    bench = black_scholes(inputs)\n",
    "    print('Prediction / Truth / Pred Mape / Black-Scholes / BS Mape(%)')\n",
    "    for i in range(num_predictions):\n",
    "        print(preds[i], '   ', truth[i], '', 100*np.abs((preds[i]-truth[i]))/truth[i],\n",
    "              ' ', bench[i], '', 100*np.abs((bench[i]-truth[i]))/truth[i])\n",
    "else:\n",
    "    print('Prediction / Truth / Pred Mape(%)')\n",
    "    for i in range(num_predictions):\n",
    "        print(preds[i], ' ', truth[i], '  ', 100*np.abs((preds[i]-truth[i]))/truth[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf-gpu)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
