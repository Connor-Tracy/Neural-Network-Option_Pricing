{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b6d42a",
   "metadata": {},
   "source": [
    "<font color=pink>\n",
    "For HyperParameter search, it can be necessary to ensure no other operations on GPU, in particular other Jupyter kernels. Run ```nvidia-smi``` in terminal to check.\n",
    "<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59394763",
   "metadata": {},
   "source": [
    "<font color=pink>\n",
    "TensorFlow makes log folder and file names so long that it exceeds the 260 character address length limit so this should be run from a directory with as short an address as possible. Tested with TF 2.3.\n",
    "<br>\n",
    "Also advisable to pause cloud storage sinking (i.e. OneDrive) as this can block the logging of trial results.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea2488",
   "metadata": {},
   "source": [
    "Estimating the amplitude and period of a sine curve given an input of 30 values\n",
    "sampled at equally spaced points along the curve over the interval [-15,15].\n",
    "The model is trained for curves of the form b+Asin(w(a+x)/2pi)\n",
    "where the period is w and the amplitude is A. The x are sampled for the\n",
    "30 equally spaced values in the interval [-15,15] as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810f81a",
   "metadata": {},
   "source": [
    "This script performs a random search or Bayesian search to identify optimal hyperparameters, and then trains a model with these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61ef80",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "#          Initialisation           #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6bd16",
   "metadata": {},
   "source": [
    "We first initialize parameters such as the number of epochs in each trial and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f24a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow.keras.backend as kb\n",
    "import kerastuner as kt\n",
    "from tensorflow import math as tm\n",
    "print(tf.config.list_physical_devices()) # Check TensorFlow can see your GPU.\n",
    "\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "np.set_printoptions(edgeitems=9, linewidth=128,\n",
    "                    formatter={'float':lambda x: f'{x:.3f}'}, precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on CPU:\n",
    "#tf.config.set_visible_devices(tf.config.experimental.list_physical_devices('CPU'))\n",
    "\n",
    "# Rounding errors on float32 can inflate the loss\n",
    "floatsize = 'float64'\n",
    "kb.set_floatx(floatsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be81412",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_pts = 30\n",
    "train_sqrt     = 450                        # Number of periods and number of amplitudes to generate in data.\n",
    "\n",
    "randSearch = True                           # True if using random hyperparameter search or hard-coding hyperparameters.\n",
    "max_trials = 60\n",
    "hyp_epochs = 20\n",
    "fit_epochs = 50\n",
    "batch      = 2**6\n",
    "val_split  = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619ac1d",
   "metadata": {},
   "source": [
    "We create training data of $train\\_sqrt^2$ curves with random periods, amplitudes and translations.\\\n",
    "Data() returns a list $[curves,\\ yAw]$ of 2 arrays:\n",
    " - $curves$ is an array in which each row is a set of $num\\_sample\\_pts$ equidistant sample points from one curve;\n",
    " - $yAw$ is an array of the period-amplitude pair for each curve sample in $curves$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_curve(period, amp, sample, aug = False):\n",
    "    return ( amp * np.sin(( sample + aug*np.random.rand() ) * (2 * np.pi / period))\n",
    "            + aug*np.random.rand() ).reshape(num_sample_pts)\n",
    "\n",
    "def data(sample_interval = [-15, 15], amp_interval = [0,30], aug = False, gridsize = train_sqrt):\n",
    "    sample = np.linspace(*sample_interval, num_sample_pts)\n",
    "    mesh   = np.meshgrid(0.05 + 10 * np.pi * np.random.rand(gridsize),          # Periods\n",
    "                         amp_interval[0] + (amp_interval[1] - amp_interval[0]) *\n",
    "                         np.random.rand(gridsize))                              # Amplitudes\n",
    "    pairs  = np.array(mesh).T.reshape(-1, 2)\n",
    "    curves = np.array([make_curve(w, a, sample,aug) for w,a in pairs])\n",
    "    return [curves, pairs]\n",
    "\n",
    "df = data(aug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_hparams(hparams):\n",
    "    keys       = hparams.keys()\n",
    "    max_length = max([len(key) for key in keys])\n",
    "    indices    = ['number_layers', 'number_units', 'learning_rate', 'rate_decay',\n",
    "                  'l1_regularizer', 'l2_regularizer', 'activation_func', 'dropout_p']\n",
    "    for key, value in hparams.items():\n",
    "        print(f'{key:<15} : {value}')\n",
    "    print('')\n",
    "\n",
    "# Terminate training after patience^th consecutive epoch with non-decreasing val_loss\n",
    "early_stopR = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20)\n",
    "early_stopB = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20)\n",
    "early_stopT = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs\\\\sinAw\\\\scalars\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M-%S\")\n",
    "tensorboard_callback = [keras.callbacks.TensorBoard(log_dir = logdir, update_freq = 'epoch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf65b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hparams):\n",
    "    if randSearch:\n",
    "        indices = hparams.keys()\n",
    "    else:\n",
    "        indices = ['number_layers', 'number_units', 'learning_rate', 'rate_decay',\n",
    "                   'l1_regularizer', 'l2_regularizer', 'activation_func', 'dropout_p']\n",
    "    number_layers, number_units, learning_rate, rate_decay, l1_reg, l2_reg, a_func, dropout_p = [hparams[ind] for ind in indices]\n",
    "\n",
    "    def get_initialiser():\n",
    "        if a_func in ['sigmoid', 'tanh']:\n",
    "            return tf.keras.initializers.glorot_uniform()\n",
    "        else:\n",
    "            return tf.keras.initializers.he_uniform()#normal()\n",
    "\n",
    "    initializer = get_initialiser()\n",
    "    regularizer = keras.regularizers.l1_l2(l1_reg, l2_reg)\n",
    "\n",
    "    norm_layer = Normalization()\n",
    "    norm_layer.adapt(df[0])\n",
    "    model = keras.Sequential()\n",
    "    model.add( InputLayer(input_shape=(num_sample_pts,), name='Input_Layer') )\n",
    "    \n",
    "    for layer in range(number_layers):\n",
    "        model.add( Dense( units             = number_units,\n",
    "                         name               = f'Layer_{layer+1}',\n",
    "                         kernel_initializer = initializer,           # Initialize weights with He/Xavier initialiser.\n",
    "                         kernel_regularizer = regularizer,\n",
    "                         activation         = a_func,\n",
    "                         dtype              = floatsize) )\n",
    "        #model.add( BatchNormalization(renorm = True) )              # Batch-norm in TF requires float32, cannot accept float64.\n",
    "        model.add( Dropout(dropout_p) )\n",
    "        \n",
    "    model.add( Dense(units = 2, activation = 'linear',               # 2 output nodes for a period-amplitude pair.\n",
    "                     dtype = floatsize, name='Output_Layer') )\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        learning_rate, decay_steps = 4000, decay_rate = rate_decay, staircase = True)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "        loss      = tf.keras.losses.MeanAbsolutePercentageError()) # Train, test loss & objective. A 'metric' is not necessary.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd9dd9",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "# Random Search\n",
    "<br>\n",
    "<font color=green>\n",
    "(TensorBoard hparams plugin hp module)     \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_range  = [2, 3]\n",
    "node_range   = [5, 16]\n",
    "l_rate_range = (np.linspace(10**-4.5, 10**-2,   600)).tolist()\n",
    "decay_range  = (np.linspace(0.82,     0.99,     400)).tolist()\n",
    "reg1_range   = (np.linspace(10**-8,   10**-5.5, 600)).tolist()\n",
    "reg2_range   = (np.linspace(10**-7,   10**-4.5, 600)).tolist()\n",
    "act_funcs    = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "dropout_ps   = (np.linspace(0.01, 0.40, 400)).tolist()\n",
    "# (10**(np.linspace(-9, -3, 600))).tolist() to sample log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd322840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HP_LAYERS = hp.HParam('number_layers', hp.IntInterval(*layer_range))\n",
    "HP_UNITS  = hp.HParam('number_units',  hp.IntInterval(*node_range))\n",
    "HP_LRATE  = hp.HParam('learning_rate',   hp.Discrete(l_rate_range))\n",
    "HP_LDECAY = hp.HParam('rate_decay',      hp.Discrete(decay_range))\n",
    "HP_REGL1  = hp.HParam('l1_regularizer',  hp.Discrete(reg1_range))\n",
    "HP_REGL2  = hp.HParam('l2_regularizer',  hp.Discrete(reg1_range))\n",
    "HP_ACTS   = hp.HParam('activation_func', hp.Discrete(act_funcs))\n",
    "HP_DROP   = hp.HParam('dropout_p', hp.Discrete(dropout_ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e76735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "trial_num = 1\n",
    "for i in range(max_trials):\n",
    "    startrun       = time.time()\n",
    "    run_name       = f'randTrial-{trial_num}'\n",
    "    logdir         = f\".\\\\sinAw\\\\random_search\\\\\" + datetime.now().strftime(\"%d-%H%M-%S\") + \"\\\\\" + run_name\n",
    "    inputs, prices = data()\n",
    "    hyperparams = {\n",
    "        'number_layers'   : HP_LAYERS.domain.sample_uniform(),\n",
    "        'number_units'    : HP_UNITS.domain.sample_uniform(),\n",
    "        'learning_rate'   : HP_LRATE.domain.sample_uniform(),\n",
    "        'rate_decay'      : HP_LDECAY.domain.sample_uniform(),\n",
    "        'l1_regularizer'  : HP_REGL1.domain.sample_uniform(),\n",
    "        'l2_regularizer'  : HP_REGL2.domain.sample_uniform(),\n",
    "        'activation_func' : HP_ACTS.domain.sample_uniform(),\n",
    "        'dropout_p'       : HP_DROP.domain.sample_uniform()}\n",
    "    \n",
    "    print(f'----Starting trial {run_name}')\n",
    "    pretty_hparams(hyperparams)\n",
    "    \n",
    "    model = model_builder(hyperparams)\n",
    "    #print(model.summary())\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(logdir), early_stopR,       # MSE+loss\n",
    "                 hp.KerasCallback(logdir + \"\\\\validation\", hyperparams)]    # TB Hparams\n",
    "    \n",
    "    train_history = model.fit(inputs, prices,\n",
    "                              validation_split = val_split,\n",
    "                              batch_size       = batch,\n",
    "                              callbacks        = callbacks,\n",
    "                              shuffle          = True,\n",
    "                              verbose          = 0,\n",
    "                              epochs           = hyp_epochs)\n",
    "    \n",
    "    print('Average val loss: ', np.average(train_history.history['val_loss'][-3:]))\n",
    "    print('Trial time taken: ', time.time()-startrun)\n",
    "    print('Cumulative time taken:', time.time()-start)\n",
    "    print('')\n",
    "    trial_num += 1\n",
    "\n",
    "print('Total time taken: ', time.time()-start)\n",
    "    \n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(1000, 250)\n",
    "    \n",
    "# An error (in prompt) of the form:\n",
    "# Blas GEMM launch failed : a.shape=(64, 30), b.shape=(30, 4), m=64, n=4, k=30\n",
    "# [[{{node sequential/Layer_1/MatMul}}]]\n",
    "# CPU->GPU Memcpy failed\n",
    "# is likely caused by another kernel also using the GPU.\n",
    "# Terminate any other active kernels before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc5c1b",
   "metadata": {},
   "source": [
    "## # Bayesian search using Keras Tuner\n",
    "<br>\n",
    "<font color=green>\n",
    "(keras-tuner HyperParameters hy_Params)     \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13cb05",
   "metadata": {},
   "source": [
    "To enable the hyperparameter search, we first create use a HyperParameter object from kerastuner, informing the algorithm of our concervative range of possible values for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeca761",
   "metadata": {},
   "outputs": [],
   "source": [
    "randSearch = False\n",
    "hy_Params = HyperParameters()\n",
    "\n",
    "hy_Params.Float(  'learning_rate',  *[10**(-3), 10**(-1)] )\n",
    "hy_Params.Float(  'rate_decay',     *[0.86, 0.94] )\n",
    "hy_Params.Float(  'l1_regularizer', *[10**(-6.5), 5*10**(-4)] )\n",
    "hy_Params.Float(  'l2_regularizer', *[10**(-6.5), 5*10**(-4)] )\n",
    "hy_Params.Choice( 'activation_func', ['elu', 'sigmoid'] )\n",
    "hy_Params.Float(  'dropout_p', *[0.1, 0.4] )\n",
    "hy_Params.Choice( 'number_layers', [2] )\n",
    "hy_Params.Int( 'number_units',  step=1, *[4, 7] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18a95e",
   "metadata": {},
   "source": [
    "We define 'tuner' by passing the model_builder function to the Keras Tuner Bayesian optimization method. Increase max_trials to increase the number of Bayesian exploration steps - the larger the better. The TensorBoard information will be stored in a dated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9fded",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logdir = f\"\\\\Bayes_search\\\\\"\n",
    "\n",
    "tuner = kt.BayesianOptimization(model_builder,\n",
    "                                hyperparameters = hy_Params,\n",
    "                                max_trials      = max_trials,\n",
    "                                objective       = 'val_loss',\n",
    "                                directory       = logdir,\n",
    "                                project_name    = \"proj\" + datetime.now().strftime(\"%d-%H%M\"))\n",
    "# The logging here saves each trial but does not itself report to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88268342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once complete, only retain summary of best trial\n",
    "import IPython\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)\n",
    "        print('Cumulative time taken:', time.time()-start)\n",
    "\n",
    "# For Bayesian optimisation we do not need to visualise the HPARAMS\n",
    "# in TensorBoard but we can still log the training curves of each run.\n",
    "cb_dir     = logdir + datetime.now().strftime(\"%d-%H%M\") + \"\\\\\"\n",
    "callbacks  = [early_stopB, ClearTrainingOutput(), tf.keras.callbacks.TensorBoard(cb_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6bbfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "inputs, prices = data()\n",
    "\n",
    "tuner.search(inputs, prices,\n",
    "             validation_split = val_split,\n",
    "             batch_size       = batch,\n",
    "             callbacks        = callbacks,\n",
    "             verbose          = 0,\n",
    "             initial_epoch    = 0,\n",
    "             shuffle          = True,\n",
    "             epochs           = hyp_epochs)\n",
    "                             \n",
    "print('Time Taken: ', time.time()-start)\n",
    "print(' ')\n",
    "    \n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(1000, 250)\n",
    "    \n",
    "# An error of the form\n",
    "\n",
    "# NotFoundError: Failed to create a NewWriteableFile:\n",
    "# logs\\sinAw\\Bayes_s999999earch\\proj25-0111\\trial_961dd8ef567fe0474969eb2fc5d39d04\\checkpoints\n",
    "# \\epoch_0\\checkpoint_temp_97e4de392a704274a8e78d72bba426de/part-00000-of-00002.data-00000-of-00001.\n",
    "# tempstate7872945028056574961 : The system cannot find the path specified.\n",
    "# ; No such process [Op:SaveV2]\n",
    "# is very likely to be due to a 'long' directory address, since TensorFlow\n",
    "# trial identifiers create addresses exceeding the 260 character limit.\n",
    "# Run TensorFlow scripts from a short directory path such as 'C:\\Music\\'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0beed",
   "metadata": {},
   "source": [
    "The tuner stores all trial hyperparameters and scores, so we now extract the hyperparameters corresponding to the lowest-loss trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcc532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_results = min(max_trials,10)\n",
    "bests = tuner.get_best_hyperparameters(num_results)\n",
    "for i in range(num_results):\n",
    "    sup = (['st', 'nd', 'rd'] + ['th'] * 7) * 5\n",
    "    sup[10:13] = ['th'] * 3\n",
    "    print(f'{i+1}{sup[i]} best hyperparameter configuration found:')\n",
    "    pretty_hparams(bests[i].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75967e49",
   "metadata": {},
   "source": [
    "## # Train best model\n",
    "<br>\n",
    "<font color=green>  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d4aa6",
   "metadata": {},
   "source": [
    "Now that we have optimized hyperparameters stored in the best_hps object, we can easily reuse $model\\_builder$ to train a full model. The TensorBoard data will be logged in a dated folder in \".\\logs\\scalars\\\" and loading TensorBoard from terminal via\n",
    "'tensorboard --logdir \"<dir/to/logs>\"'\n",
    "will open a localhost server. Navigate to the address and observe the loss vs epochs plot updating every 30 seconds. Note if the foldername is constant (ie not recording date and time) then the search will continue with knowledge of the previous runs. I find TensorBoard is much more stable launching from Terminal than a notebook however so this is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = f\".\\\\scalars\\\\\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=logdir + datetime.now().strftime(\"%d-%H%M-%S\"), update_freq='epoch')\n",
    "randSearch = True\n",
    "\n",
    "if randSearch:\n",
    "    best_hp = { # Copy hparams from printout then add string-quotes and commas.\n",
    "        'number_layers'   : 2,\n",
    "        'number_units'    : 12,\n",
    "        'learning_rate'   : 0.004871010080685016,\n",
    "        'rate_decay'      : 0.9032167916319965,\n",
    "        'l1_regularizer'  : 0.0003417837964467978,\n",
    "        'l2_regularizer'  : 0.000390735809379687,\n",
    "        'activation_func' : 'sigmoid',\n",
    "        'dropout'         : 0.23402950096803992 }\n",
    "else:\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "model = model_builder(best_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a668bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sqrt = 200\n",
    "df = data()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "training_history = model.fit(*df,\n",
    "                             batch_size = batch,\n",
    "                             epochs = fit_epochs,\n",
    "                             verbose = 0,\n",
    "                             validation_split = 0.2,\n",
    "                             callbacks = [early_stopT, tensorboard_cb])\n",
    "                             \n",
    "print(\"Average test loss: \", np.average(training_history.history['loss'][-3:]))\n",
    "print(\"Average val loss: \",  np.average(training_history.history['val_loss'][-3:]))\n",
    "print('Time taken: ', time.time()-start)\n",
    "\n",
    "import winsound\n",
    "for i in range(2):\n",
    "    winsound.Beep(800, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"trained_sine_model\")\n",
    "#saved_model = keras.models.load_model(\"trained_sine_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fa6fa",
   "metadata": {},
   "source": [
    "## # 3D Error Surface Plot\n",
    "<br>\n",
    "<font color=green>\n",
    "Plotting can take a little while.     \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Magic command so we can interact with a 3D plot in the notebook.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "fig = plt.figure(figsize = (12,10))\n",
    "ax = plt.axes(projection = '3d')\n",
    "\n",
    "df_test = data(aug = False, gridsize = 300)\n",
    "\n",
    "def plot_err_3d(save = False):\n",
    "    errors           = np.abs(df_test[1] - saved_model.predict(df_test[0])).T\n",
    "    mapes            = 100 * errors / df_test[1].T\n",
    "    mapes            = np.sum(mapes, axis=0) / 2\n",
    "    mapes[mapes>100] = 100\n",
    "    surf             = ax.plot_trisurf(*df_test[1].T, mapes, cmap = cm.coolwarm,\n",
    "                                       linewidth = 0, antialiased = False)\n",
    "    \n",
    "    #ax.set_title('Plot of MAPE Vs Amplitude & Period', pad=0.5)\n",
    "    ax.set_xlabel('Period', fontsize = 11, labelpad = 10)\n",
    "    ax.set_ylabel('Amplitude', fontsize = 11, labelpad = 10)\n",
    "    ax.set_zlabel('MAPE (%)', fontsize = 11, labelpad = 10)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(11))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.00f'))\n",
    "    ax.view_init(20, 60)    \n",
    "    fig.colorbar(surf, shrink = 0.5, aspect = 5)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('3d_err_plot.png', transparent = True, dpi = 200, bbox_inches = 'tight')\n",
    "    fig.show()\n",
    "    \n",
    "plot_err_3d(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481469d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf-gpu)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
